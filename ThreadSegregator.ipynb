{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of UnreadKings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gNE91jpwRm4",
        "colab_type": "text"
      },
      "source": [
        "# NLP Project\n",
        "### Authors:\n",
        "- 01FB16ECS014 - Abhishek Sinha\n",
        "- 01FB16ECS208 - Midhush Manohar T.K.\n",
        "- 01FB16ECS396 - Srikumar Subramanian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T3PWXnJxZ7q",
        "colab_type": "code",
        "outputId": "7ef3a4c5-d54b-41ed-f7eb-54bda1e91da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ledtjEcRz2lV",
        "colab_type": "code",
        "outputId": "f4d33447-f2fc-4a86-a57c-184e92718f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install neuralcoref\n",
        "!pip install -U spacy==2.1.3\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy link en_core_web_lg en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neuralcoref\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/24/0ec7845a5b73b637aa691ff4d1b9b48f3a0f3369f4002a59ffd7a7462fdb/neuralcoref-4.0-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.9.224)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.16.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.1.8)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (1.12.224)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.1.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->neuralcoref) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->neuralcoref) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.1.0->neuralcoref) (4.28.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.224->boto3->neuralcoref) (1.12.0)\n",
            "Installing collected packages: neuralcoref\n",
            "Successfully installed neuralcoref-4.0\n",
            "Collecting spacy==2.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/da/3a1c54694c2d2f40df82f38a19ae14c6eb24a5a1a0dae87205ebea7a84d8/spacy-2.1.3-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.2.4)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.1.0)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (7.0.8)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2019.6.16)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.3) (4.28.1)\n",
            "Installing collected packages: spacy\n",
            "  Found existing installation: spacy 2.1.8\n",
            "    Uninstalling spacy-2.1.8:\n",
            "      Successfully uninstalled spacy-2.1.8\n",
            "Successfully installed spacy-2.1.3\n",
            "Collecting en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 131.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255076 sha256=0c274d441d7da75553a5ff2a3531bbb8c037022788ec12523d399c1464297d08\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zt9qzpqx/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXDc7lTQz5cg",
        "colab_type": "code",
        "outputId": "8af464ef-0f15-4dff-cc32-3dd3307d732b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import spacy\n",
        "# nlp = spacy.load('en_core_web_lg')\n",
        "nlp = spacy.load('en')\n",
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp, greedyness=0.6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40155833/40155833 [00:00<00:00, 72346254.86B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7fcf58e25208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQOOzieoz8uy",
        "colab_type": "code",
        "outputId": "1976cc4c-140b-49b5-ac7d-911275ff963f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48RWSHht0vHy",
        "colab_type": "code",
        "outputId": "b5169495-633f-4e75-ff82-18d631855449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIGi69uCXLqZ",
        "colab_type": "code",
        "outputId": "5cd7937d-bd23-4ea5-a9a1-22b5153fc785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ls \"drive/My Drive/NLP Project/CSVs\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compressed_output_complicated.csv      compressed_output_simple.csv\n",
            "compressed_output_simple_changed.csv   compressed_output_simple_time.csv\n",
            "compressed_output_simple_clean.csv     compressed_output_threads.csv\n",
            "compressed_output_simple_clean.gsheet  data.csv\n",
            "compressed_output_simple_clean.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdhGjQPvP7xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install autocorrect\n",
        "#from autocorrect import spell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9DEJQRY2OjI",
        "colab_type": "code",
        "outputId": "1c8178ea-398d-4e7a-fa3b-a0214c580c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "import io\n",
        "df = pd.read_csv('drive/My Drive/NLP Project/CSVs/compressed_output_simple_clean.csv')\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>SequenceNo</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Speaker</th>\n",
              "      <th>Text</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>GT_Thread</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>01/06/19</td>\n",
              "      <td>06:54 PM</td>\n",
              "      <td>MESSAGE</td>\n",
              "      <td>Messages to this group are now secured with en...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>09:52 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>What time are the tests ?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:01 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>2:30-3:30 Test 2-3:30 written</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:09 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>For computer science alone ? Or all ?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:10 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>No clue</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  SequenceNo      Date  ... GT_Thread Unnamed: 11 Unnamed: 12\n",
              "0           0           0  01/06/19  ...         1         NaN         NaN\n",
              "1           1           1  16/02/19  ...         2         NaN         NaN\n",
              "2           2           2  16/02/19  ...         2         NaN         NaN\n",
              "3           3           4  16/02/19  ...         2         NaN         NaN\n",
              "4           4           5  16/02/19  ...         2         NaN         NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhNJYRBZ7nr4",
        "colab_type": "code",
        "outputId": "5d1420d3-063b-4de9-a5aa-9acf7b8a36f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Creating ground truth values\n",
        "\n",
        "sentence_threads = list(df['GT_Thread'])\n",
        "num_threads = max(sentence_threads)\n",
        "num_sentences = len(sentence_threads)\n",
        "\n",
        "gt_threads = [[] for i in range(num_threads-1)]\n",
        "\n",
        "#print(sentence_threads)\n",
        "#print(gt_threads)\n",
        "for i in range(1,len(sentence_threads)):\n",
        "     gt_threads[sentence_threads[i]-2].append(i)\n",
        "    \n",
        "for i in gt_threads:\n",
        "     print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "[17, 18, 19, 20, 21]\n",
            "[22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
            "[71, 72, 73, 74, 75]\n",
            "[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "[90, 91, 92]\n",
            "[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]\n",
            "[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128]\n",
            "[121]\n",
            "[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]\n",
            "[143, 144, 145, 146, 147, 148, 149, 150]\n",
            "[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]\n",
            "[171, 172, 173, 174, 175, 176, 177, 178, 179]\n",
            "[180, 181, 182, 183, 184]\n",
            "[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]\n",
            "[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]\n",
            "[211, 212, 213, 214, 215, 216, 217, 218]\n",
            "[219, 220, 221, 222, 223, 224, 225, 226]\n",
            "[227, 228, 229, 230, 231, 232, 233, 234, 235, 236]\n",
            "[243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
            "[256, 257]\n",
            "[258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271]\n",
            "[237, 238, 239, 240, 241, 242]\n",
            "[24, 25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIEL8eRtHO3a",
        "colab_type": "code",
        "outputId": "c8fb01bc-6ee1-4a5f-8a67-e45bddceb5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#import string\n",
        "import re\n",
        "\n",
        "#porter = nltk.PorterStemmer()\n",
        "wnlem = nltk.WordNetLemmatizer()\n",
        "df['TextMod']=df['Text'].apply(lambda x: \" \".join(x.split(\"-\")))\n",
        "df['TextMod']=df['TextMod'].apply(lambda x: re.sub(r\"([0-9]{1,2}[: ][0-9]{2}(( *am)|( *pm))?)|([0-9]{1,2}(( *am)|( *pm)))\", \"time\", x))\n",
        "df['Text_words']=df['TextMod'].apply(nltk.word_tokenize)\n",
        "eng_stop_words = stopwords.words('english')\n",
        "eng_stop_words.append(\"n't\")\n",
        "\n",
        "print(eng_stop_words)\n",
        "#df['Text_imp_words']=df['Text_words'].apply(lambda x: [spell(porter.stem(i.lower())) for i in x if(i not in eng_stop_words and re.match(r\"[0-9a-zA-Z:-]+\",i)) ])  #stemming\n",
        "df['Tagged']=df['Text_words'].apply(nltk.pos_tag) \n",
        "eng_stop_words=list(set(eng_stop_words)-set(['what', 'which', 'who', 'whom',\"when\",\"where\"]))\n",
        "\n",
        "df['Text_imp_words']=df['Text_words'].apply(lambda x: [wnlem.lemmatize(i.lower()) for i in x if(i.lower() not in eng_stop_words and re.match(r\"^[0-9a-zA-Z:]+$\",i))]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"n't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grbMb1YlHZq9",
        "colab_type": "code",
        "outputId": "a030d754-ac8e-4d6a-8407-63048c384d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "df['ThreadNo']=0\n",
        "df.at[0,\"ThreadNo\"]=1\n",
        "df.head(8)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>SequenceNo</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Speaker</th>\n",
              "      <th>Text</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>GT_Thread</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "      <th>TextMod</th>\n",
              "      <th>Text_words</th>\n",
              "      <th>Tagged</th>\n",
              "      <th>Text_imp_words</th>\n",
              "      <th>ThreadNo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>01/06/19</td>\n",
              "      <td>06:54 PM</td>\n",
              "      <td>MESSAGE</td>\n",
              "      <td>Messages to this group are now secured with en...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Messages to this group are now secured with en...</td>\n",
              "      <td>[Messages, to, this, group, are, now, secured,...</td>\n",
              "      <td>[(Messages, NNS), (to, TO), (this, DT), (group...</td>\n",
              "      <td>[message, group, secured, end, end, encryption...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>09:52 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>What time are the tests ?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What time are the tests ?</td>\n",
              "      <td>[What, time, are, the, tests, ?]</td>\n",
              "      <td>[(What, WP), (time, NN), (are, VBP), (the, DT)...</td>\n",
              "      <td>[what, time, test]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:01 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>2:30-3:30 Test 2-3:30 written</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>time time Test 2 time written</td>\n",
              "      <td>[time, time, Test, 2, time, written]</td>\n",
              "      <td>[(time, NN), (time, NN), (Test, NNP), (2, CD),...</td>\n",
              "      <td>[time, time, test, 2, time, written]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:09 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>For computer science alone ? Or all ?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For computer science alone ? Or all ?</td>\n",
              "      <td>[For, computer, science, alone, ?, Or, all, ?]</td>\n",
              "      <td>[(For, IN), (computer, NN), (science, NN), (al...</td>\n",
              "      <td>[computer, science, alone]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:10 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>No clue</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No clue</td>\n",
              "      <td>[No, clue]</td>\n",
              "      <td>[(No, DT), (clue, NN)]</td>\n",
              "      <td>[clue]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:10 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>I'll ask</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'll ask</td>\n",
              "      <td>[I, 'll, ask]</td>\n",
              "      <td>[(I, PRP), ('ll, MD), (ask, VB)]</td>\n",
              "      <td>[ask]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:12 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>Okay</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Okay</td>\n",
              "      <td>[Okay]</td>\n",
              "      <td>[(Okay, NN)]</td>\n",
              "      <td>[okay]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:57 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>Ec ee me 3:45-4:45</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ec ee me time time</td>\n",
              "      <td>[Ec, ee, me, time, time]</td>\n",
              "      <td>[(Ec, NNP), (ee, VB), (me, PRP), (time, NN), (...</td>\n",
              "      <td>[ec, ee, time, time]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  ThreadNo\n",
              "0           0  ...         1\n",
              "1           1  ...         0\n",
              "2           2  ...         0\n",
              "3           3  ...         0\n",
              "4           4  ...         0\n",
              "5           5  ...         0\n",
              "6           6  ...         0\n",
              "7           7  ...         0\n",
              "\n",
              "[8 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1ZZXU20wUTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def validTimeDif(d1,d2,t1,t2):\n",
        "    maxDif=360\n",
        "    \n",
        "    dateRegex='(.{1,2})\\/(.{2})\\/(.{2})'\n",
        "    timeRegex= '(.{1,2}):(.{1,2}) ((AM|am)|(PM|pm))'\n",
        "    \n",
        "    dm1 = re.match(dateRegex, d1)\n",
        "    dm2 = re.match(dateRegex, d2)\n",
        "    \n",
        "    m1 = re.match(timeRegex, t1)\n",
        "    m2 = re.match(timeRegex, t2)\n",
        "\n",
        "    hr1 = int(m1.group(1))\n",
        "    min1 = int(m1.group(2))\n",
        "    pod1 = (m1.group(3).lower()==\"pm\")\n",
        "    if (hr1 == 12 and pod1 == 1):\n",
        "        time1 = hr1\n",
        "    else:\n",
        "        time1 = (hr1+(pod1*12))%24\n",
        "    hr2 = int(m2.group(1))\n",
        "    min2 = int(m2.group(2))\n",
        "    pod2 = (m2.group(3).lower()==\"pm\")\n",
        "    if (hr2 == 12 and pod2 == 1):\n",
        "        time2 = hr2\n",
        "    else:\n",
        "        time2 = (hr2+(pod2*12))%24\n",
        "    day1= int(dm1.group(1))\n",
        "    month1=int(dm1.group(2))\n",
        "    y1=int(dm1.group(3))\n",
        "\n",
        "    day2= int(dm2.group(1))\n",
        "    month2=int(dm2.group(2))\n",
        "    y2=int(dm2.group(3))\n",
        "    \n",
        "    date1 =datetime(int(\"20\"+str(y1)),month1,day1,time1,min1)\n",
        "    date2= datetime(int(\"20\"+str(y2)),month2,day2,time2,min2)\n",
        "    dif=date2-date1\n",
        "    dif=dif.total_seconds()/60    \n",
        "    if(dif<maxDif):\n",
        "        return 1 #can be in same thread\n",
        "    return 0             #can't be in the same thread\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i1mqiIoavU1",
        "colab_type": "code",
        "outputId": "11c2af9b-6023-44d1-8a99-4864ae499710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "wVmodel = KeyedVectors.load_word2vec_format('drive/My Drive/NLP Project/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd01v6PSl1eB",
        "colab_type": "code",
        "outputId": "a8769b3e-476f-4011-ceba-124daa2e8e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "dictionary=wVmodel.wv.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shfhoUfrmMg-",
        "colab_type": "code",
        "outputId": "631a8b75-72ae-4e13-9f26-fb4bea69ccfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "df[\"vector\"] = df[\"Text_imp_words\"].apply(lambda x: np.average(np.array([wVmodel[i] for i in x if i in dictionary]),axis=0))\n",
        "df[\"vector\"]=df[\"vector\"].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:392: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pknYW4-Spjgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def cosine_similarity(v1,v2):\n",
        "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "    for i in range(len(v1)):\n",
        "        x = v1[i]; y = v2[i]\n",
        "        sumxx += x*x\n",
        "        sumyy += y*y\n",
        "        sumxy += x*y\n",
        "    return sumxy/math.sqrt(sumxx*sumyy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjJylSNsNH86",
        "colab_type": "code",
        "outputId": "1373c015-578f-476f-f037-c1ba31c670ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "thread_flag = 0\n",
        "\n",
        "threads=[{\"threadNo\": 1,\n",
        "          \"vocab\": set(df.iloc[0].loc[\"Text_imp_words\"]),\n",
        "          \"sentenceNo\": [0],\n",
        "          \"ended\": 0\n",
        "        }]\n",
        "\n",
        "end_count = 0\n",
        "\n",
        "newThreadNo=2\n",
        "for index, row in df.iloc[1:,:].iterrows():\n",
        "  #  print(index)\n",
        "    initial_probabilities=[0 for i in range(len(threads))]\n",
        "    commonWords=set()\n",
        "    max_com=0\n",
        "    max_i=0\n",
        "    allNew=1 #flag to check if any threads are alive(time constraint)\n",
        "    threadSimilarities=[]\n",
        "    \n",
        "    \n",
        "    # Test Function to get the respective pronoun references\n",
        "    # def getPronounReferences(doc):\n",
        "    #     prons = []\n",
        "    #     refs = []\n",
        "    #     print('\\nPronouns and their references:')\n",
        "    #     for i in doc._.coref_scores:\n",
        "    #         try:    \n",
        "    #             doc._.coref_scores[i].pop(i)\n",
        "    #         except:\n",
        "    #             continue\n",
        "    #     for token in doc:\n",
        "    #         if token.pos_ == 'PRON' and token._.in_coref:\n",
        "    #             print(token._.coref_clusters)\n",
        "    #             for cluster in token._.coref_clusters:\n",
        "    #                 prons.append(token.text)\n",
        "    #                 refs.append(cluster.main.text)\n",
        "    #     return prons, refs\n",
        "\n",
        "    max_coref_score = -1 # acts as threshold\n",
        "    def corefCheck(sentence):\n",
        "        global max_coref_score\n",
        "        doc = nlp(sentence)\n",
        "        coref_scores = []\n",
        "        for i in doc._.coref_scores:\n",
        "            for j in doc._.coref_scores[i]:\n",
        "                if (i == j):\n",
        "                    doc._.coref_scores[i][j] = float(\"-inf\")\n",
        "            # doc._.coref_scores[i].pop(i,2)\n",
        "        for i in doc._.coref_scores:\n",
        "            if (bool(doc._.coref_scores[i])):\n",
        "                try:\n",
        "                    coref_scores.append(max(doc._.coref_scores[i].values()))\n",
        "                except:\n",
        "                    pass\n",
        "        try:\n",
        "            if (max_coref_score < max(coref_scores)):\n",
        "                max_coref_score = max(coref_scores)\n",
        "        except:\n",
        "            pass\n",
        "        if doc._.has_coref:\n",
        "            return max_coref_score\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    max_ans = -1\n",
        "    max_index = -1\n",
        "    \n",
        "    #for m in range(min(index, 15)):\n",
        "    if index - 15>=0:\n",
        "        lookback_index = index - 15\n",
        "    else:\n",
        "        lookback_index = 0\n",
        "    \n",
        "    for m in range(index-1, lookback_index, -1):\n",
        "        ans = corefCheck(df.iloc[index].loc['TextMod'] + df.iloc[m].loc['TextMod'])\n",
        "        if (ans > max_ans):\n",
        "            max_ans = ans\n",
        "            max_index = df.at[m, \"ThreadNo\"] - 1\n",
        "            break\n",
        "            # Breaking at this point cause we are starting from the back and using the first match that we get\n",
        "            #print(df.at[m, \"ThreadNo\"])\n",
        "    \n",
        "    if (max_ans > 10.0):\n",
        "        most_similar= max_index #threadSimilarities.index(max(threadSimilarities))\n",
        "        #print(most_similar)\n",
        "        #print(len(threads))\n",
        "        threads[most_similar][\"vocab\"]= threads[most_similar][\"vocab\"].union(set(row[\"Text_imp_words\"]))\n",
        "        threads[most_similar][\"sentenceNo\"].append(index)\n",
        "        df.at[index,\"ThreadNo\"]=threads[most_similar][\"threadNo\"] \n",
        "    \n",
        "    else:\n",
        "        for i in threads:\n",
        "            if(not i[\"ended\"]):\n",
        "            # print(\"vocab\",i[\"vocab\"])\n",
        "            # print(row[\"Date\"],df.iloc[i[\"sentenceNo\"][len(i[\"sentenceNo\"])-1]][\"Date\"],row[\"Time\"],df.iloc[i[\"sentenceNo\"][len(i[\"sentenceNo\"])-1]][\"Time\"])\n",
        "        \n",
        "                if(validTimeDif(df.iloc[i[\"sentenceNo\"][len(i[\"sentenceNo\"])-1]][\"Date\"],row[\"Date\"],df.iloc[i[\"sentenceNo\"][len(i[\"sentenceNo\"])-1]][\"Time\"],row[\"Time\"])): #time constraint\n",
        "                    allNew=0\n",
        "                    com_words=i[\"vocab\"].intersection(set(row[\"Text_imp_words\"]))\n",
        "                    similarities=[0]\n",
        "                    for sentenceNos in i[\"sentenceNo\"]:\n",
        "                    #  print(\"this\",sentenceNos)\n",
        "                        if(index-sentenceNos<15 and type(row[\"vector\"])!=type(0) and type(df.iloc[sentenceNos].loc[\"vector\"])!=type(0)):  #15 message sliding window\n",
        "                            similarities.append(cosine_similarity(row[\"vector\"],df.iloc[sentenceNos].loc[\"vector\"]))\n",
        "                        else: \n",
        "                            similarities.append(0)\n",
        "                    avg_thread_similarity= np.mean(similarities)\n",
        "                    threadSimilarities.append(avg_thread_similarity)\n",
        "                #    print(com_words)\n",
        "                    commonWords.union(com_words)\n",
        "                    if(( (len(com_words)+1) /(len(i[\"vocab\"])+1)) > max_com):\n",
        "                        max_com= len(com_words)\n",
        "                        max_i=i\n",
        "                else:\n",
        "                    threadSimilarities.append(0)\n",
        "                    end_count += 1 # Counting the number of times thread has ended\n",
        "                    i[\"ended\"]=1\n",
        "            else:\n",
        "                threadSimilarities.append(0)\n",
        "    # print(\"threadSimilarities\",len(threadSimilarities))\n",
        "    if(allNew==0):\n",
        "        if(len(row[\"Text_words\"])<=3 or len(row[\"Text_imp_words\"])==0):     # Min new thread sentence length >3 + 1 imp word atleast\n",
        "                ThreadNo=df.iloc[index-1].loc[\"ThreadNo\"]\n",
        "                for i in threads:\n",
        "                    if(i[\"threadNo\"]==ThreadNo):\n",
        "                        thread_flag = 1\n",
        "                        threads[(threads.index(i))][\"vocab\"].union(row[\"Text_imp_words\"])\n",
        "                        threads[(threads.index(i))][\"sentenceNo\"].append(index)\n",
        "                        df.at[index,\"ThreadNo\"]=(ThreadNo)\n",
        "        \n",
        "        else:  \n",
        "            if(max(threadSimilarities)>0.10):      #Threshold for thread similarity (Assumption 4?)\n",
        "                most_similar=threadSimilarities.index(max(threadSimilarities))\n",
        "                threads[most_similar][\"vocab\"]= threads[most_similar][\"vocab\"].union(set(row[\"Text_imp_words\"]))\n",
        "                threads[most_similar][\"sentenceNo\"].append(index)\n",
        "                df.at[index,\"ThreadNo\"]=threads[most_similar][\"threadNo\"]\n",
        "            else:\n",
        "                if(max_com==0):\n",
        "                    threads.append({\n",
        "                    \"threadNo\": newThreadNo,\n",
        "                    \"vocab\": set(row[\"Text_imp_words\"]),\n",
        "                    \"sentenceNo\": [index],\n",
        "                    \"ended\": 0\n",
        "                    })\n",
        "                    df.at[index,\"ThreadNo\"]=newThreadNo\n",
        "                    newThreadNo+=1\n",
        "                else:\n",
        "                    max_i[\"vocab\"]= max_i[\"vocab\"].union(set(row[\"Text_imp_words\"]))\n",
        "                    max_i[\"sentenceNo\"].append(index)\n",
        "                    df.at[index,\"ThreadNo\"]=threads[threads.index(max_i)][\"threadNo\"]\n",
        "    else:\n",
        "        threads.append({\n",
        "        \"threadNo\": newThreadNo,\n",
        "        \"vocab\": set(row[\"Text_imp_words\"]),\n",
        "        \"sentenceNo\": [index],\n",
        "        \"ended\": 0\n",
        "        })\n",
        "        df.at[index,\"ThreadNo\"]=newThreadNo\n",
        "        newThreadNo+=1\n",
        "        \n",
        "print(thread_flag)\n",
        "print(end_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tz6qSVH3cAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iKuLoBaxU5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# buckets = []\n",
        "# noB = set(df.iloc[:,10])\n",
        "# for i in noB:\n",
        "#     bucket = list(df.loc[df['ThreadNo'] == i].iloc[:,0])\n",
        "#     buckets.append(bucket)\n",
        "# for i in buckets:\n",
        "#     print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3WXG16iScWi",
        "colab_type": "code",
        "outputId": "3a4f7bd1-6f35-4fd1-dcca-18079509f0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "print(len(threads))\n",
        "pred_threads = []\n",
        "for x in threads:\n",
        "    pred_threads.append(x[\"sentenceNo\"])\n",
        "    print(x[\"sentenceNo\"])\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "[17, 18, 19, 20, 21]\n",
            "[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
            "[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
            "[71, 72, 73, 74, 75]\n",
            "[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "[90]\n",
            "[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]\n",
            "[107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128]\n",
            "[112]\n",
            "[121]\n",
            "[129, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]\n",
            "[130, 131]\n",
            "[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]\n",
            "[168, 169, 170]\n",
            "[190]\n",
            "[191, 192, 193, 194, 195, 196]\n",
            "[197]\n",
            "[198, 209, 210]\n",
            "[199, 200, 201, 202, 203, 204, 205, 206, 207, 208]\n",
            "[211, 212, 213, 214, 215, 216, 217, 218]\n",
            "[219, 220, 221, 222, 223, 224, 225, 226]\n",
            "[227, 228, 229, 230, 231, 232, 233, 234, 235, 236]\n",
            "[237, 238, 239, 240, 241, 242]\n",
            "[243, 244, 245, 248, 249, 250, 251, 252, 253, 254, 255]\n",
            "[246, 247]\n",
            "[256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdcT0pcAnnsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dist_error(x, y, total_num):    \n",
        "    set_x = [set(t) for t in x]\n",
        "    set_y = [set(t) for t in y]\n",
        "\n",
        "    if(len(x) < len(y)):\n",
        "        r = len(x)\n",
        "        c = len(y)\n",
        "        pair_res = [set_x, set_y]\n",
        "    else:\n",
        "        r = len(y)\n",
        "        c = len(x)\n",
        "        pair_res = [set_y, set_x]\n",
        "\n",
        "    sim_list = []\n",
        "    for i in range(len(pair_res[0])):\n",
        "        sim_list.append([])\n",
        "        for j in range(len(pair_res[1])):\n",
        "            sim_list[i].append( len( pair_res[0][i].symmetric_difference(pair_res[1][j]) ) ) \n",
        "\n",
        "    sim_mat = np.array([np.array(xi) for xi in sim_list])\n",
        "\n",
        "    col_marks = [0 for i in range(c)]\n",
        "    row_marks = [0 for i in range(c)]\n",
        "    sum_dist = 0\n",
        "    for i in range(r):\n",
        "        found_flag = 0\n",
        "        temp_sorted = sorted(sim_list[i])\n",
        "        for ascend_index in range(len(temp_sorted)):\n",
        "            val = temp_sorted[ascend_index]\n",
        "            val_indices = np.where(sim_mat[i] == val)[0]\n",
        "            for val_ind in val_indices:\n",
        "                if(col_marks[val_ind]==0):\n",
        "                    sum_dist += val\n",
        "                    col_marks[val_ind] = 1\n",
        "                    row_marks[val_ind] = i\n",
        "                    found_flag = 1\n",
        "                    break\n",
        "                elif(col_marks[val_ind]==1):\n",
        "                    og_row = row_marks[val_ind]\n",
        "                    if (sim_list[og_row][val_ind] > val):\n",
        "                        for gap in range(r):\n",
        "                            if col_marks[gap]==0 and sim_list[og_row][gap]+val < sim_list[og_row][val_ind] + temp_sorted[ascend_index+1]:\n",
        "                                sum_dist -= sim_list[og_row][val_ind]\n",
        "                                sum_dist += sim_list[og_row][gap]\n",
        "                                sum_dist += val\n",
        "                                row_marks[gap] = og_row\n",
        "                                row_marks[val_ind] = i\n",
        "                                found_flag = 1\n",
        "                                break\n",
        "                \n",
        "            if found_flag == 1:\n",
        "                break\n",
        "    \n",
        "    print(\"\\nSim Matrix\")\n",
        "    for i in sim_list:\n",
        "        print(i)\n",
        "        \n",
        "    return sum_dist/(2*total_num)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg74uM0ooE9Q",
        "colab_type": "code",
        "outputId": "6bb56992-2945-4bc5-eade-dc904df58319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Getting results\n",
        "print(\"predicted threads\")\n",
        "for i in pred_threads:\n",
        "    print(i)\n",
        "\n",
        "print(\"\\nGround Truth\")\n",
        "for i in gt_threads:\n",
        "    print(i)\n",
        "        \n",
        "#print((find_dist_error(pred_threads, gt_threads, num_sentences)*num_threads)/num_sentences)\n",
        "print(find_dist_error(pred_threads, gt_threads, num_sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted threads\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "[17, 18, 19, 20, 21]\n",
            "[22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
            "[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
            "[71, 72, 73, 74, 75]\n",
            "[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "[90]\n",
            "[91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]\n",
            "[107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128]\n",
            "[112]\n",
            "[121]\n",
            "[129, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146]\n",
            "[130, 131]\n",
            "[147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]\n",
            "[168, 169, 170]\n",
            "[190]\n",
            "[191, 192, 193, 194, 195, 196]\n",
            "[197]\n",
            "[198, 209, 210]\n",
            "[199, 200, 201, 202, 203, 204, 205, 206, 207, 208]\n",
            "[211, 212, 213, 214, 215, 216, 217, 218]\n",
            "[219, 220, 221, 222, 223, 224, 225, 226]\n",
            "[227, 228, 229, 230, 231, 232, 233, 234, 235, 236]\n",
            "[237, 238, 239, 240, 241, 242]\n",
            "[243, 244, 245, 248, 249, 250, 251, 252, 253, 254, 255]\n",
            "[246, 247]\n",
            "[256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271]\n",
            "\n",
            "Ground Truth\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "[17, 18, 19, 20, 21]\n",
            "[22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
            "[71, 72, 73, 74, 75]\n",
            "[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "[90, 91, 92]\n",
            "[93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]\n",
            "[107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128]\n",
            "[121]\n",
            "[129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142]\n",
            "[143, 144, 145, 146, 147, 148, 149, 150]\n",
            "[151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]\n",
            "[171, 172, 173, 174, 175, 176, 177, 178, 179]\n",
            "[180, 181, 182, 183, 184]\n",
            "[185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196]\n",
            "[197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210]\n",
            "[211, 212, 213, 214, 215, 216, 217, 218]\n",
            "[219, 220, 221, 222, 223, 224, 225, 226]\n",
            "[227, 228, 229, 230, 231, 232, 233, 234, 235, 236]\n",
            "[243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
            "[256, 257]\n",
            "[258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271]\n",
            "[237, 238, 239, 240, 241, 242]\n",
            "[24, 25]\n",
            "\n",
            "Sim Matrix\n",
            "[1, 21, 53, 28, 21, 30, 17, 32, 36, 17, 17, 32, 18, 56, 19, 17, 22, 17, 19, 26, 24, 24, 26, 22, 27, 18, 32]\n",
            "[22, 0, 42, 17, 10, 19, 6, 21, 25, 6, 6, 21, 7, 45, 8, 6, 11, 6, 8, 15, 13, 13, 15, 11, 16, 7, 21]\n",
            "[64, 52, 14, 35, 52, 61, 48, 63, 67, 48, 48, 63, 49, 87, 50, 48, 53, 48, 50, 57, 55, 55, 57, 53, 58, 49, 63]\n",
            "[22, 10, 42, 17, 0, 19, 6, 21, 25, 6, 6, 21, 7, 45, 8, 6, 11, 6, 8, 15, 13, 13, 15, 11, 16, 7, 21]\n",
            "[31, 19, 51, 26, 19, 0, 15, 30, 34, 15, 15, 30, 16, 54, 17, 15, 20, 15, 17, 24, 22, 22, 24, 20, 25, 16, 30]\n",
            "[20, 8, 40, 15, 8, 17, 2, 15, 23, 4, 4, 19, 5, 43, 6, 4, 9, 4, 6, 13, 11, 11, 13, 9, 14, 5, 19]\n",
            "[31, 19, 51, 26, 19, 28, 15, 2, 34, 15, 15, 30, 16, 54, 17, 15, 20, 15, 17, 24, 22, 22, 24, 20, 25, 16, 30]\n",
            "[38, 26, 58, 33, 26, 35, 22, 37, 1, 20, 22, 37, 23, 61, 24, 22, 27, 22, 24, 31, 29, 29, 31, 27, 32, 23, 37]\n",
            "[18, 6, 38, 13, 6, 15, 2, 17, 21, 2, 0, 17, 3, 41, 4, 2, 7, 2, 4, 11, 9, 9, 11, 7, 12, 3, 17]\n",
            "[31, 19, 51, 26, 19, 28, 15, 30, 34, 15, 15, 6, 12, 54, 17, 15, 20, 15, 17, 24, 22, 22, 24, 20, 25, 16, 30]\n",
            "[25, 13, 45, 20, 13, 22, 9, 24, 28, 9, 9, 16, 10, 40, 11, 9, 14, 9, 11, 18, 16, 16, 18, 14, 19, 10, 24]\n",
            "[37, 25, 57, 32, 25, 34, 21, 36, 40, 21, 21, 36, 22, 26, 17, 21, 26, 21, 23, 30, 28, 28, 30, 26, 31, 22, 36]\n",
            "[26, 14, 46, 21, 14, 23, 10, 25, 29, 10, 10, 25, 11, 31, 12, 10, 15, 10, 12, 19, 17, 17, 19, 15, 20, 11, 25]\n",
            "[22, 10, 42, 17, 10, 19, 6, 21, 25, 6, 6, 21, 7, 35, 8, 6, 11, 6, 8, 15, 13, 13, 15, 11, 16, 7, 21]\n",
            "[29, 17, 49, 24, 17, 26, 13, 28, 32, 13, 13, 28, 14, 42, 15, 11, 6, 13, 15, 22, 20, 20, 22, 18, 23, 14, 28]\n",
            "[31, 19, 51, 26, 19, 28, 15, 30, 34, 15, 15, 30, 16, 54, 17, 15, 20, 13, 11, 4, 22, 22, 24, 20, 25, 16, 30]\n",
            "[25, 13, 45, 20, 13, 22, 9, 24, 28, 9, 9, 24, 10, 48, 11, 9, 14, 9, 11, 18, 0, 16, 18, 14, 19, 10, 24]\n",
            "[25, 13, 45, 20, 13, 22, 9, 24, 28, 9, 9, 24, 10, 48, 11, 9, 14, 9, 11, 18, 16, 0, 18, 14, 19, 10, 24]\n",
            "[27, 15, 47, 22, 15, 24, 11, 26, 30, 11, 11, 26, 12, 50, 13, 11, 16, 11, 13, 20, 18, 18, 0, 16, 21, 12, 26]\n",
            "[30, 18, 50, 25, 18, 27, 14, 29, 33, 14, 14, 29, 15, 53, 16, 14, 19, 14, 16, 23, 21, 21, 23, 19, 2, 11, 29]\n",
            "[19, 7, 39, 14, 7, 16, 3, 18, 22, 3, 3, 18, 4, 42, 5, 3, 8, 3, 5, 12, 10, 10, 12, 8, 13, 4, 14]\n",
            "[31, 19, 51, 26, 19, 28, 15, 30, 34, 15, 15, 30, 16, 54, 17, 15, 20, 15, 17, 24, 22, 22, 24, 20, 25, 16, 2]\n",
            "[23, 11, 43, 18, 11, 20, 7, 22, 26, 7, 7, 22, 8, 46, 9, 7, 12, 7, 9, 16, 14, 14, 16, 0, 17, 8, 22]\n",
            "[19, 7, 35, 14, 7, 16, 3, 18, 22, 3, 3, 18, 4, 42, 5, 3, 8, 3, 5, 12, 10, 10, 12, 8, 13, 4, 18]\n",
            "0.16544117647058823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUyWw77rshml",
        "colab_type": "code",
        "outputId": "95ecfdfe-3f24-40e4-ba7b-1bc1a9904643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df.iloc[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>SequenceNo</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Speaker</th>\n",
              "      <th>Text</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>GT_Thread</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "      <th>TextMod</th>\n",
              "      <th>Text_words</th>\n",
              "      <th>Tagged</th>\n",
              "      <th>Text_imp_words</th>\n",
              "      <th>ThreadNo</th>\n",
              "      <th>vector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>01/06/19</td>\n",
              "      <td>06:54 PM</td>\n",
              "      <td>MESSAGE</td>\n",
              "      <td>Messages to this group are now secured with en...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Messages to this group are now secured with en...</td>\n",
              "      <td>[Messages, to, this, group, are, now, secured,...</td>\n",
              "      <td>[(Messages, NNS), (to, TO), (this, DT), (group...</td>\n",
              "      <td>[message, group, secured, end, end, encryption...</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.008464813, 0.0040302277, -0.06251526, 0.03...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>09:52 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>What time are the tests ?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What time are the tests ?</td>\n",
              "      <td>[What, time, are, the, tests, ?]</td>\n",
              "      <td>[(What, WP), (time, NN), (are, VBP), (the, DT)...</td>\n",
              "      <td>[what, time, test]</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.016764322, 0.04815674, 0.117614746, 0.0615...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:01 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>2:30-3:30 Test 2-3:30 written</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>time time Test 2 time written</td>\n",
              "      <td>[time, time, Test, 2, time, written]</td>\n",
              "      <td>[(time, NN), (time, NN), (Test, NNP), (2, CD),...</td>\n",
              "      <td>[time, time, test, 2, time, written]</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.048746746, 0.056599934, 0.057200115, 0.062...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:09 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>For computer science alone ? Or all ?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For computer science alone ? Or all ?</td>\n",
              "      <td>[For, computer, science, alone, ?, Or, all, ?]</td>\n",
              "      <td>[(For, IN), (computer, NN), (science, NN), (al...</td>\n",
              "      <td>[computer, science, alone]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.11726888, -0.051757812, 0.1595866, 0.221679...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:10 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>No clue</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No clue</td>\n",
              "      <td>[No, clue]</td>\n",
              "      <td>[(No, DT), (clue, NN)]</td>\n",
              "      <td>[clue]</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.013977051, -0.29492188, -0.04663086, 0.041...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:10 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>I'll ask</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'll ask</td>\n",
              "      <td>[I, 'll, ask]</td>\n",
              "      <td>[(I, PRP), ('ll, MD), (ask, VB)]</td>\n",
              "      <td>[ask]</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.028076172, 0.05859375, 0.18945312, 0.09765...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:12 AM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>Okay</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Okay</td>\n",
              "      <td>[Okay]</td>\n",
              "      <td>[(Okay, NN)]</td>\n",
              "      <td>[okay]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.08154297, -0.063964844, 0.025268555, 0.1445...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:57 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>Ec ee me 3:45-4:45</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ec ee me time time</td>\n",
              "      <td>[Ec, ee, me, time, time]</td>\n",
              "      <td>[(Ec, NNP), (ee, VB), (me, PRP), (time, NN), (...</td>\n",
              "      <td>[ec, ee, time, time]</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.103881836, 0.15411377, 0.0652771, 0.177978...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>10:57 AM</td>\n",
              "      <td>Abhishek</td>\n",
              "      <td>Written everyone is 2-3:30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Written everyone is 2 time</td>\n",
              "      <td>[Written, everyone, is, 2, time]</td>\n",
              "      <td>[(Written, NNP), (everyone, NN), (is, VBZ), (2...</td>\n",
              "      <td>[written, everyone, 2, time]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.004760742, -0.040893555, 0.023757935, 0.085...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>16/02/19</td>\n",
              "      <td>01:51 PM</td>\n",
              "      <td>Midhush</td>\n",
              "      <td>Ah</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ah</td>\n",
              "      <td>[Ah]</td>\n",
              "      <td>[(Ah, NN)]</td>\n",
              "      <td>[ah]</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.1640625, 0.19726562, 0.19921875, 0.1884765...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                             vector\n",
              "0           0  ...  [-0.008464813, 0.0040302277, -0.06251526, 0.03...\n",
              "1           1  ...  [-0.016764322, 0.04815674, 0.117614746, 0.0615...\n",
              "2           2  ...  [-0.048746746, 0.056599934, 0.057200115, 0.062...\n",
              "3           3  ...  [0.11726888, -0.051757812, 0.1595866, 0.221679...\n",
              "4           4  ...  [-0.013977051, -0.29492188, -0.04663086, 0.041...\n",
              "5           5  ...  [-0.028076172, 0.05859375, 0.18945312, 0.09765...\n",
              "6           6  ...  [0.08154297, -0.063964844, 0.025268555, 0.1445...\n",
              "7           7  ...  [-0.103881836, 0.15411377, 0.0652771, 0.177978...\n",
              "8           8  ...  [0.004760742, -0.040893555, 0.023757935, 0.085...\n",
              "9           9  ...  [-0.1640625, 0.19726562, 0.19921875, 0.1884765...\n",
              "\n",
              "[10 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_izJEQDz1NQ",
        "colab_type": "code",
        "outputId": "dbe2133b-de12-424a-ff13-b27b618c379a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "print(cosine_similarity(df.iloc[4].loc[\"vector\"],df.iloc[3].loc[\"vector\"]),df.iloc[4].loc[\"Text\"],df.iloc[3].loc[\"Text\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08549606605647238 No clue For CS alone ? Or all ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSvkAC7qqS2a",
        "colab_type": "code",
        "outputId": "b9a97801-1058-4f9f-e649-500b4e93cc53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "w1=\"tea\"\n",
        "w2=\"coffee\"\n",
        "sent1=[i for i in \"when is it\".lower().split() if i not in eng_stop_words]\n",
        "print(sent1)\n",
        "sent2=[i for i in \"at time\".lower().split() if i not in eng_stop_words]\n",
        "if ((set(sent1).issubset(set(dictionary))) and (set(sent2).issubset(set(dictionary)))):\n",
        "    print(sent1,sent2)\n",
        "    print(wVmodel.n_similarity(sent1, sent2))  \n",
        "elif ((set(sent1).intersection(set(dictionary))==set(sent1))):\n",
        "    print(sent2,\"not present\");\n",
        "    print(set(sent2)-set(dictionary))\n",
        "elif((set(sent2).intersection(set(dictionary))==set(sent2))):\n",
        "    print(sent1,\"not present\")\n",
        "else:\n",
        "    print(\"both absent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['when']\n",
            "['when'] ['time']\n",
            "0.44044927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-nu8cBJqWRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "text = '12:45 am'\n",
        "\n",
        "m = re.match('(.{1,2}):(.{1,2}) ((am)|(pm))', text)\n",
        "if m:\n",
        "    found = m.group(1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}